nohup: 忽略输入
epoch,train_loss,val_loss,arc_acc,lab_acc
loading data...
# train sentences 31864
# val sentences 7967
creating model...
/data/zhangyu/anaconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.33 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
BiAffineParser(
  (word_embed): TimeDistributed(
    (module): Embedding(19706, 100)
  )
  (pos_embed): TimeDistributed(
    (module): Embedding(48, 28)
  )
  (emb_drop): Dropout(p=0.33)
  (lstm): LSTM(128, 512, batch_first=True, dropout=0.33, bidirectional=True)
  (arc_mlp_h): TimeDistributed(
    (module): MLP(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.33)
        )
      )
    )
  )
  (arc_mlp_d): TimeDistributed(
    (module): MLP(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.33)
        )
      )
    )
  )
  (lab_mlp_h): TimeDistributed(
    (module): MLP(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=1024, out_features=128, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.33)
        )
      )
    )
  )
  (lab_mlp_d): TimeDistributed(
    (module): MLP(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=1024, out_features=128, bias=True)
          (1): ReLU()
          (2): Dropout(p=0.33)
        )
      )
    )
  )
  (arc_attn): BiAffineAttn(
    (U): torch.Size([1, 256, 257])
  )
  (lab_attn): BiAffineAttn(
    (U): torch.Size([49, 129, 129])
  )
)
Epoch  0.0 train_loss  6.228 val_loss  4.191 arc_acc  0.612 lab_acc  0.504 time  109.9 sec
 0.000,  6.228,  4.191,  0.612,  0.504
Epoch  0.1 train_loss  3.320 val_loss  2.125 arc_acc  0.781 lab_acc  0.692 time  108.7 sec
 0.100,  3.320,  2.125,  0.781,  0.692
Epoch  0.2 train_loss  2.017 val_loss  1.306 arc_acc  0.821 lab_acc  0.818 time  109.4 sec
 0.200,  2.017,  1.306,  0.821,  0.818
Epoch  0.3 train_loss  1.448 val_loss  0.955 arc_acc  0.845 lab_acc  0.884 time  110.3 sec
 0.300,  1.448,  0.955,  0.845,  0.884
Epoch  0.4 train_loss  1.170 val_loss  0.821 arc_acc  0.858 lab_acc  0.910 time  108.7 sec
 0.400,  1.170,  0.821,  0.858,  0.910
Epoch  0.5 train_loss  1.049 val_loss  0.746 arc_acc  0.865 lab_acc  0.922 time  110.8 sec
 0.500,  1.049,  0.746,  0.865,  0.922
Epoch  0.6 train_loss  0.985 val_loss  0.694 arc_acc  0.872 lab_acc  0.928 time  113.4 sec
 0.600,  0.985,  0.694,  0.872,  0.928
Epoch  0.7 train_loss  0.920 val_loss  0.654 arc_acc  0.877 lab_acc  0.935 time  126.1 sec
 0.700,  0.920,  0.654,  0.877,  0.935
Epoch  0.8 train_loss  0.843 val_loss  0.618 arc_acc  0.883 lab_acc  0.940 time  123.9 sec
 0.800,  0.843,  0.618,  0.883,  0.940
Epoch  0.9 train_loss  0.814 val_loss  0.598 arc_acc  0.885 lab_acc  0.944 time  121.5 sec
 0.900,  0.814,  0.598,  0.885,  0.944
Epoch  1.0 train_loss  0.741 val_loss  0.579 arc_acc  0.890 lab_acc  0.946 time  126.1 sec
 1.000,  0.741,  0.579,  0.890,  0.946
Epoch  1.1 train_loss  0.708 val_loss  0.559 arc_acc  0.893 lab_acc  0.948 time  124.5 sec
 1.100,  0.708,  0.559,  0.893,  0.948
Epoch  1.2 train_loss  0.716 val_loss  0.540 arc_acc  0.896 lab_acc  0.950 time  120.7 sec
 1.200,  0.716,  0.540,  0.896,  0.950
Epoch  1.3 train_loss  0.682 val_loss  0.538 arc_acc  0.897 lab_acc  0.952 time  120.4 sec
 1.300,  0.682,  0.538,  0.897,  0.952
Epoch  1.4 train_loss  0.661 val_loss  0.517 arc_acc  0.899 lab_acc  0.954 time  124.6 sec
 1.400,  0.661,  0.517,  0.899,  0.954
Epoch  1.5 train_loss  0.681 val_loss  0.506 arc_acc  0.901 lab_acc  0.954 time  121.9 sec
 1.500,  0.681,  0.506,  0.901,  0.954
Epoch  1.6 train_loss  0.657 val_loss  0.502 arc_acc  0.903 lab_acc  0.955 time  124.3 sec
 1.600,  0.657,  0.502,  0.903,  0.955
Epoch  1.7 train_loss  0.651 val_loss  0.493 arc_acc  0.902 lab_acc  0.957 time  128.1 sec
 1.700,  0.651,  0.493,  0.902,  0.957
Epoch  1.8 train_loss  0.625 val_loss  0.488 arc_acc  0.903 lab_acc  0.957 time  122.7 sec
 1.800,  0.625,  0.488,  0.903,  0.957
Epoch  1.9 train_loss  0.599 val_loss  0.475 arc_acc  0.904 lab_acc  0.958 time  122.6 sec
 1.900,  0.599,  0.475,  0.904,  0.958
Epoch  2.0 train_loss  0.594 val_loss  0.475 arc_acc  0.908 lab_acc  0.959 time  126.5 sec
 2.000,  0.594,  0.475,  0.908,  0.959
Epoch  2.1 train_loss  0.554 val_loss  0.479 arc_acc  0.909 lab_acc  0.960 time  125.3 sec
 2.100,  0.554,  0.479,  0.909,  0.960
Epoch  2.2 train_loss  0.541 val_loss  0.469 arc_acc  0.908 lab_acc  0.960 time  117.8 sec
 2.200,  0.541,  0.469,  0.908,  0.960
Epoch  2.3 train_loss  0.548 val_loss  0.478 arc_acc  0.909 lab_acc  0.960 time  121.4 sec
 2.300,  0.548,  0.478,  0.909,  0.960
Epoch  2.4 train_loss  0.552 val_loss  0.462 arc_acc  0.910 lab_acc  0.961 time  121.2 sec
 2.400,  0.552,  0.462,  0.910,  0.961
Epoch  2.5 train_loss  0.553 val_loss  0.461 arc_acc  0.910 lab_acc  0.961 time  127.4 sec
 2.500,  0.553,  0.461,  0.910,  0.961
Epoch  2.6 train_loss  0.566 val_loss  0.448 arc_acc  0.912 lab_acc  0.962 time  119.9 sec
 2.600,  0.566,  0.448,  0.912,  0.962
Epoch  2.7 train_loss  0.546 val_loss  0.447 arc_acc  0.914 lab_acc  0.962 time  125.6 sec
 2.700,  0.546,  0.447,  0.914,  0.962
Epoch  2.8 train_loss  0.550 val_loss  0.437 arc_acc  0.913 lab_acc  0.962 time  118.4 sec
 2.800,  0.550,  0.437,  0.913,  0.962
Epoch  2.9 train_loss  0.541 val_loss  0.431 arc_acc  0.914 lab_acc  0.963 time  122.2 sec
 2.900,  0.541,  0.431,  0.914,  0.963
Epoch  3.0 train_loss  0.529 val_loss  0.439 arc_acc  0.914 lab_acc  0.963 time  132.3 sec
 3.000,  0.529,  0.439,  0.914,  0.963
Epoch  3.1 train_loss  0.490 val_loss  0.447 arc_acc  0.914 lab_acc  0.964 time  121.7 sec
 3.100,  0.490,  0.447,  0.914,  0.964
Epoch  3.2 train_loss  0.499 val_loss  0.436 arc_acc  0.914 lab_acc  0.964 time  126.5 sec
 3.200,  0.499,  0.436,  0.914,  0.964
Epoch  3.3 train_loss  0.484 val_loss  0.435 arc_acc  0.915 lab_acc  0.964 time  121.8 sec
 3.300,  0.484,  0.435,  0.915,  0.964
Epoch  3.4 train_loss  0.503 val_loss  0.425 arc_acc  0.915 lab_acc  0.965 time  122.8 sec
 3.400,  0.503,  0.425,  0.915,  0.965
Epoch  3.5 train_loss  0.487 val_loss  0.439 arc_acc  0.916 lab_acc  0.965 time  120.0 sec
 3.500,  0.487,  0.439,  0.916,  0.965
Epoch  3.6 train_loss  0.485 val_loss  0.431 arc_acc  0.916 lab_acc  0.965 time  127.9 sec
 3.600,  0.485,  0.431,  0.916,  0.965
Epoch  3.7 train_loss  0.484 val_loss  0.427 arc_acc  0.917 lab_acc  0.965 time  124.0 sec
 3.700,  0.484,  0.427,  0.917,  0.965
Epoch  3.8 train_loss  0.488 val_loss  0.423 arc_acc  0.918 lab_acc  0.966 time  123.8 sec
 3.800,  0.488,  0.423,  0.918,  0.966
Epoch  3.9 train_loss  0.500 val_loss  0.416 arc_acc  0.918 lab_acc  0.966 time  122.2 sec
 3.900,  0.500,  0.416,  0.918,  0.966
Epoch  4.0 train_loss  0.468 val_loss  0.434 arc_acc  0.918 lab_acc  0.967 time  123.8 sec
 4.000,  0.468,  0.434,  0.918,  0.967
Epoch  4.1 train_loss  0.444 val_loss  0.441 arc_acc  0.919 lab_acc  0.966 time  128.7 sec
 4.100,  0.444,  0.441,  0.919,  0.966
Epoch  4.2 train_loss  0.455 val_loss  0.427 arc_acc  0.919 lab_acc  0.967 time  128.9 sec
 4.200,  0.455,  0.427,  0.919,  0.967
Epoch  4.3 train_loss  0.455 val_loss  0.434 arc_acc  0.918 lab_acc  0.967 time  128.9 sec
 4.300,  0.455,  0.434,  0.918,  0.967
Epoch  4.4 train_loss  0.439 val_loss  0.425 arc_acc  0.919 lab_acc  0.967 time  125.8 sec
 4.400,  0.439,  0.425,  0.919,  0.967
Epoch  4.5 train_loss  0.455 val_loss  0.416 arc_acc  0.919 lab_acc  0.967 time  126.6 sec
 4.500,  0.455,  0.416,  0.919,  0.967
Epoch  4.6 train_loss  0.459 val_loss  0.416 arc_acc  0.919 lab_acc  0.967 time  125.1 sec
 4.600,  0.459,  0.416,  0.919,  0.967
Epoch  4.7 train_loss  0.454 val_loss  0.425 arc_acc  0.919 lab_acc  0.968 time  131.8 sec
 4.700,  0.454,  0.425,  0.919,  0.968
Epoch  4.8 train_loss  0.463 val_loss  0.411 arc_acc  0.919 lab_acc  0.968 time  126.9 sec
 4.800,  0.463,  0.411,  0.919,  0.968
Epoch  4.9 train_loss  0.455 val_loss  0.410 arc_acc  0.920 lab_acc  0.968 time  118.4 sec
 4.900,  0.455,  0.410,  0.920,  0.968
Epoch  5.0 train_loss  0.452 val_loss  0.418 arc_acc  0.921 lab_acc  0.968 time  112.4 sec
 5.000,  0.452,  0.418,  0.921,  0.968
Epoch  5.1 train_loss  0.409 val_loss  0.415 arc_acc  0.921 lab_acc  0.968 time  114.9 sec
 5.100,  0.409,  0.415,  0.921,  0.968
Epoch  5.2 train_loss  0.410 val_loss  0.425 arc_acc  0.920 lab_acc  0.969 time  112.7 sec
 5.200,  0.410,  0.425,  0.920,  0.969
Epoch  5.3 train_loss  0.414 val_loss  0.423 arc_acc  0.921 lab_acc  0.969 time  111.2 sec
 5.300,  0.414,  0.423,  0.921,  0.969
Epoch  5.4 train_loss  0.424 val_loss  0.429 arc_acc  0.922 lab_acc  0.968 time  113.3 sec
 5.400,  0.424,  0.429,  0.922,  0.968
Epoch  5.5 train_loss  0.449 val_loss  0.415 arc_acc  0.922 lab_acc  0.969 time  111.7 sec
 5.500,  0.449,  0.415,  0.922,  0.969
Epoch  5.6 train_loss  0.438 val_loss  0.408 arc_acc  0.922 lab_acc  0.969 time  109.6 sec
 5.600,  0.438,  0.408,  0.922,  0.969
Epoch  5.7 train_loss  0.431 val_loss  0.406 arc_acc  0.922 lab_acc  0.969 time  112.3 sec
 5.700,  0.431,  0.406,  0.922,  0.969
Epoch  5.8 train_loss  0.426 val_loss  0.407 arc_acc  0.921 lab_acc  0.969 time  108.2 sec
 5.800,  0.426,  0.407,  0.921,  0.969
Epoch  5.9 train_loss  0.443 val_loss  0.402 arc_acc  0.922 lab_acc  0.969 time  108.8 sec
 5.900,  0.443,  0.402,  0.922,  0.969
Epoch  6.0 train_loss  0.423 val_loss  0.412 arc_acc  0.922 lab_acc  0.969 time  112.6 sec
 6.000,  0.423,  0.412,  0.922,  0.969
Epoch  6.1 train_loss  0.396 val_loss  0.423 arc_acc  0.923 lab_acc  0.969 time  115.8 sec
 6.100,  0.396,  0.423,  0.923,  0.969
Epoch  6.2 train_loss  0.388 val_loss  0.430 arc_acc  0.922 lab_acc  0.970 time  116.9 sec
 6.200,  0.388,  0.430,  0.922,  0.970
Epoch  6.3 train_loss  0.393 val_loss  0.418 arc_acc  0.923 lab_acc  0.970 time  119.4 sec
 6.300,  0.393,  0.418,  0.923,  0.970
Epoch  6.4 train_loss  0.408 val_loss  0.419 arc_acc  0.923 lab_acc  0.970 time  117.6 sec
 6.400,  0.408,  0.419,  0.923,  0.970
Epoch  6.5 train_loss  0.400 val_loss  0.417 arc_acc  0.922 lab_acc  0.970 time  114.3 sec
 6.500,  0.400,  0.417,  0.922,  0.970
Epoch  6.6 train_loss  0.409 val_loss  0.418 arc_acc  0.923 lab_acc  0.969 time  118.0 sec
 6.600,  0.409,  0.418,  0.923,  0.969
Epoch  6.7 train_loss  0.431 val_loss  0.419 arc_acc  0.922 lab_acc  0.970 time  118.1 sec
 6.700,  0.431,  0.419,  0.922,  0.970
Epoch  6.8 train_loss  0.417 val_loss  0.402 arc_acc  0.922 lab_acc  0.970 time  116.0 sec
 6.800,  0.417,  0.402,  0.922,  0.970
Epoch    68: reducing learning rate of group 0 to 8.0000e-04.
Epoch    68: reducing learning rate of group 1 to 8.0000e-04.
Epoch    68: reducing learning rate of group 2 to 4.0000e-05.
Epoch  6.9 train_loss  0.394 val_loss  0.404 arc_acc  0.924 lab_acc  0.970 time  115.8 sec
 6.900,  0.394,  0.404,  0.924,  0.970
Epoch  7.0 train_loss  0.396 val_loss  0.401 arc_acc  0.925 lab_acc  0.970 time  118.6 sec
 7.000,  0.396,  0.401,  0.925,  0.970
Epoch  7.1 train_loss  0.343 val_loss  0.411 arc_acc  0.926 lab_acc  0.971 time  112.8 sec
 7.100,  0.343,  0.411,  0.926,  0.971
Epoch  7.2 train_loss  0.353 val_loss  0.404 arc_acc  0.926 lab_acc  0.971 time  120.8 sec
 7.200,  0.353,  0.404,  0.926,  0.971
Epoch  7.3 train_loss  0.343 val_loss  0.423 arc_acc  0.926 lab_acc  0.971 time  117.1 sec
 7.300,  0.343,  0.423,  0.926,  0.971
Epoch  7.4 train_loss  0.335 val_loss  0.411 arc_acc  0.927 lab_acc  0.971 time  121.0 sec
 7.400,  0.335,  0.411,  0.927,  0.971
Epoch  7.5 train_loss  0.351 val_loss  0.410 arc_acc  0.926 lab_acc  0.971 time  119.7 sec
 7.500,  0.351,  0.410,  0.926,  0.971
Epoch  7.6 train_loss  0.339 val_loss  0.401 arc_acc  0.927 lab_acc  0.971 time  114.5 sec
 7.600,  0.339,  0.401,  0.927,  0.971
Epoch  7.7 train_loss  0.334 val_loss  0.395 arc_acc  0.927 lab_acc  0.971 time  116.4 sec
 7.700,  0.334,  0.395,  0.927,  0.971
Epoch  7.8 train_loss  0.339 val_loss  0.409 arc_acc  0.927 lab_acc  0.971 time  117.5 sec
 7.800,  0.339,  0.409,  0.927,  0.971
Epoch  7.9 train_loss  0.342 val_loss  0.401 arc_acc  0.928 lab_acc  0.971 time  117.4 sec
 7.900,  0.342,  0.401,  0.928,  0.971
Epoch  8.0 train_loss  0.334 val_loss  0.417 arc_acc  0.928 lab_acc  0.971 time  117.3 sec
 8.000,  0.334,  0.417,  0.928,  0.971
Epoch  8.1 train_loss  0.330 val_loss  0.415 arc_acc  0.928 lab_acc  0.971 time  115.5 sec
 8.100,  0.330,  0.415,  0.928,  0.971
Epoch  8.2 train_loss  0.320 val_loss  0.410 arc_acc  0.927 lab_acc  0.971 time  119.3 sec
 8.200,  0.320,  0.410,  0.927,  0.971
Epoch  8.3 train_loss  0.328 val_loss  0.424 arc_acc  0.928 lab_acc  0.972 time  118.4 sec
 8.300,  0.328,  0.424,  0.928,  0.972
Epoch  8.4 train_loss  0.315 val_loss  0.416 arc_acc  0.928 lab_acc  0.972 time  118.9 sec
 8.400,  0.315,  0.416,  0.928,  0.972
Epoch  8.5 train_loss  0.319 val_loss  0.403 arc_acc  0.928 lab_acc  0.972 time  120.0 sec
 8.500,  0.319,  0.403,  0.928,  0.972
Epoch  8.6 train_loss  0.331 val_loss  0.422 arc_acc  0.928 lab_acc  0.972 time  118.8 sec
 8.600,  0.331,  0.422,  0.928,  0.972
Epoch    86: reducing learning rate of group 0 to 3.2000e-04.
Epoch    86: reducing learning rate of group 1 to 3.2000e-04.
Epoch    86: reducing learning rate of group 2 to 1.6000e-05.
Epoch  8.7 train_loss  0.313 val_loss  0.410 arc_acc  0.929 lab_acc  0.972 time  119.9 sec
 8.700,  0.313,  0.410,  0.929,  0.972
Epoch  8.8 train_loss  0.329 val_loss  0.403 arc_acc  0.929 lab_acc  0.972 time  122.9 sec
 8.800,  0.329,  0.403,  0.929,  0.972
Epoch  8.9 train_loss  0.305 val_loss  0.405 arc_acc  0.929 lab_acc  0.972 time  120.3 sec
 8.900,  0.305,  0.405,  0.929,  0.972
Epoch  9.0 train_loss  0.301 val_loss  0.398 arc_acc  0.930 lab_acc  0.972 time  121.2 sec
 9.000,  0.301,  0.398,  0.930,  0.972
Epoch  9.1 train_loss  0.300 val_loss  0.420 arc_acc  0.930 lab_acc  0.972 time  123.7 sec
 9.100,  0.300,  0.420,  0.930,  0.972
Epoch  9.2 train_loss  0.292 val_loss  0.420 arc_acc  0.930 lab_acc  0.972 time  125.4 sec
 9.200,  0.292,  0.420,  0.930,  0.972
Epoch  9.3 train_loss  0.303 val_loss  0.405 arc_acc  0.930 lab_acc  0.972 time  122.6 sec
 9.300,  0.303,  0.405,  0.930,  0.972
Epoch  9.4 train_loss  0.296 val_loss  0.405 arc_acc  0.930 lab_acc  0.972 time  121.0 sec
 9.400,  0.296,  0.405,  0.930,  0.972
Epoch  9.5 train_loss  0.282 val_loss  0.421 arc_acc  0.930 lab_acc  0.972 time  122.2 sec
 9.500,  0.282,  0.421,  0.930,  0.972
Epoch    95: reducing learning rate of group 0 to 1.2800e-04.
Epoch    95: reducing learning rate of group 1 to 1.2800e-04.
Epoch    95: reducing learning rate of group 2 to 6.4000e-06.
Epoch  9.6 train_loss  0.287 val_loss  0.413 arc_acc  0.930 lab_acc  0.972 time  129.3 sec
 9.600,  0.287,  0.413,  0.930,  0.972
Epoch  9.7 train_loss  0.299 val_loss  0.415 arc_acc  0.930 lab_acc  0.972 time  146.9 sec
 9.700,  0.299,  0.415,  0.930,  0.972
Epoch  9.8 train_loss  0.297 val_loss  0.413 arc_acc  0.930 lab_acc  0.972 time  157.0 sec
 9.800,  0.297,  0.413,  0.930,  0.972
Epoch  9.9 train_loss  0.295 val_loss  0.413 arc_acc  0.930 lab_acc  0.972 time  152.0 sec
 9.900,  0.295,  0.413,  0.930,  0.972
Done!
